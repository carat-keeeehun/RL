{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3C.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqppydH/g2izqFJ8q18V+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keeeehun/RL/blob/main/A3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_IFW0bR3WvR"
      },
      "source": [
        "!pip install ray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjmrb9Zpc-dx"
      },
      "source": [
        "import ray\n",
        "import gym\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from gym import spaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY6ReYx9dAZ0"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP7G5ARTfRVj"
      },
      "source": [
        "#Policy & Value function\n",
        "\n",
        "본 실습에서 사용할 CartPole 환경의 policy(actor)와 value function(critic)을 나타내는 뉴럴넷 모델입니다.\n",
        "\n",
        "\\\\\n",
        "\n",
        "## Policy network\n",
        "- get_action_logprob 함수를 통해 action을 샘플링하고 그 action에 대한 log probability도 같이 반환해줍니다.\n",
        "\n",
        "## Value network\n",
        "- get_value 함수를 통해 value 값을 얻을 수 있습니다.\n",
        "\n",
        "## 두 network 공통으로,\n",
        "- get_weights 함수를 통해 weight 값(파라미터 값)을 얻을 수 있습니다.\n",
        "- get_grad 함수를 통해 파라미터들의 gradient 값을 얻을 수 있습니다.\n",
        "- set_grad 함수를 통해 파라미터들에게 gradient를 설정해줄 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OtmDySxe_8y"
      },
      "source": [
        "class CartpolePolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, device):\n",
        "        super(CartpolePolicy, self).__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        self.l1 = nn.Linear(obs_dim, 64)\n",
        "        self.l2 = nn.Linear(64, 128)\n",
        "        self.l3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        x = F.softmax(x, dim=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_action_logprob(self, obs):\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        obs = torch.unsqueeze(obs, 0)\n",
        "        obs = obs.to(self.device)\n",
        "        output = self.forward(obs)\n",
        "        categorical = Categorical(output)\n",
        "        action = categorical.sample()\n",
        "        logprob = categorical.log_prob(action)\n",
        "\n",
        "        return action.item(), logprob\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = []\n",
        "        for param in self.parameters():\n",
        "            weights.append(param.data.clone())\n",
        "    \n",
        "        return weights\n",
        "    \n",
        "    def get_grad(self):\n",
        "        grads = []\n",
        "        for name, param in self.named_parameters():\n",
        "            grads.append(param.grad)\n",
        "    \n",
        "        return grads\n",
        "    \n",
        "    def set_grad(self, grads):\n",
        "        for target_param, grad in zip(self.parameters(), grads):\n",
        "            target_param.grad.data.copy_(grad.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ8KOCvZfCya"
      },
      "source": [
        "class CartpoleCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, device):\n",
        "        super(CartpoleCritic, self).__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.device = device\n",
        "\n",
        "        self.l1 = nn.Linear(obs_dim, 64)\n",
        "        self.l2 = nn.Linear(64, 128)\n",
        "        self.l3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def get_value(self, obs):\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        obs = torch.unsqueeze(obs, 0)\n",
        "        obs = obs.to(self.device)\n",
        "        output = self.forward(obs)\n",
        "\n",
        "        return output\n",
        "    \n",
        "    def get_weights(self):\n",
        "        weights = []\n",
        "        for param in self.parameters():\n",
        "            weights.append(param.data.clone())\n",
        "    \n",
        "        return weights\n",
        "    \n",
        "    def get_grad(self):\n",
        "        grads = []\n",
        "        for name, param in self.named_parameters():\n",
        "            grads.append(param.grad)\n",
        "    \n",
        "        return grads\n",
        "    \n",
        "    def set_grad(self, grads):\n",
        "        for target_param, grad in zip(self.parameters(), grads):\n",
        "            target_param.grad.data.copy_(grad.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY7X97MfgTlr"
      },
      "source": [
        "# Test\n",
        "\n",
        "- 하나의 에피소드를 실행하면서 policy network가 잘 학습되고 있는지를 확인하는 함수입니다.\n",
        "- 해당 에피소드의 각 step에서의 reward를 모두 합하여 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMbA6bvQgPEc"
      },
      "source": [
        "def test(pi_model, env):\n",
        "    test_episode_reward = 0\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action, _ = pi_model.get_action_logprob(obs)\n",
        "        next_obs, rew, done, _ = env.step(action)\n",
        "        test_episode_reward += rew\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return test_episode_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIg3a0gghRE"
      },
      "source": [
        "# Plot\n",
        "- test의 reward를 그래프로 보여주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH7hTO34gfDC"
      },
      "source": [
        "def plot(test_rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.title('test reward')\n",
        "    plt.plot(test_rewards)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjmI3v9Fgrbf"
      },
      "source": [
        "#Worker agent\n",
        "\n",
        "- Ray를 사용하는 class는 앞에 @ray.remote라는 annotation을 사용하여 정의합니다.\n",
        "- 여러개의 class를 만들어내어 worker로 사용하며, 이 때 각 worker가 사용하는 resource를 임의로 정해줄 수 있습니다. ( 본 예제에서는 cuda(gpu)를 사용하기 위해 num_gpus를 설정했습니다. )\n",
        "\n",
        "## compute_gradient 함수\n",
        "- compute_gradient 함수는 A3C의 핵심이 되는 함수입니다.\n",
        "- 다른 worker들로부터 업데이트 된 global pi와 global v의 weight를 받아서, 해당 worker의 weight를 업데이트 해주고, gradient를 계산해주는 역할입니다.\n",
        "- backward()를 여러번 호출하여 gradient를 누적시키고, 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMbWdYb2grL6"
      },
      "source": [
        "ray.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPVdG3FGguvk"
      },
      "source": [
        "@ray.remote(num_gpus=0.5)\n",
        "class A3CAgent():\n",
        "    def __init__(self, env_name, actor_id, device):\n",
        "        self.id = actor_id\n",
        "\n",
        "        # Make env\n",
        "        self.env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "        # shape\n",
        "        obs_shape = env.observation_space.shape\n",
        "        act_shape = tuple([1]) # int to Tuple\n",
        "        obs_dim = obs_shape[0]\n",
        "        n_actions = env.action_space.n\n",
        "        buffer_size = 1000\n",
        "\n",
        "        # Define NN\n",
        "        self.pi = CartpolePolicy(obs_dim=obs_dim, n_actions=n_actions, device=device)\n",
        "        self.v  = CartpoleCritic(obs_dim=obs_dim, device=device)\n",
        "        self.pi.to(device)\n",
        "        self.v.to(device)\n",
        "\n",
        "        # Optimizer to make gradient 0 after computing gradient\n",
        "        self.pi_optimizer = optim.Adam(self.pi.parameters())\n",
        "        self.v_optimizer = optim.Adam(self.v.parameters())\n",
        "\n",
        "        self.t_max = 200\n",
        "        self.gamma = 0.99\n",
        "    \n",
        "    def compute_gradient(self, global_pi_params, global_v_params):\n",
        "        self.pi.train()\n",
        "        self.v.train()\n",
        "\n",
        "        # Synchronize thread-specific parameters\n",
        "        for target_param, param in zip(self.pi.parameters(), global_pi_params):\n",
        "            target_param.data.copy_(param.data)\n",
        "        for target_param, param in zip(self.v.parameters(), global_v_params):\n",
        "            target_param.data.copy_(param.data)\n",
        "        \n",
        "        logprob_list = []\n",
        "        reward_list = []\n",
        "        obs_list = []\n",
        "\n",
        "        # rollout until t_max or done\n",
        "        obs = env.reset()\n",
        "        for t in range(self.t_max):\n",
        "            action, logprob = self.pi.get_action_logprob(obs)\n",
        "            next_obs, rew, done, _ = env.step(action)\n",
        "            logprob_list.append(logprob)\n",
        "            reward_list.append(rew)\n",
        "            obs_list.append(obs)\n",
        "            obs = next_obs\n",
        "            if done:\n",
        "                obs = env.reset()\n",
        "                break\n",
        "        # R\n",
        "        if done:\n",
        "            R = 0\n",
        "        else:\n",
        "            R = self.v.get_value(next_obs).item()\n",
        "        \n",
        "        # Reset gradients\n",
        "        self.pi_optimizer.zero_grad()\n",
        "        self.v_optimizer.zero_grad()\n",
        "\n",
        "        for i in range(len(reward_list)-2, -1, -1): # reverse\n",
        "            R = reward_list[i] + self.gamma * R\n",
        "            # accumulate gradient of pi\n",
        "            pi_advantage = R - self.v.get_value(obs_list[i]).item()\n",
        "            pi_loss = -1 * logprob_list[i] * pi_advantage\n",
        "            pi_loss.backward()\n",
        "\n",
        "            v_advantage = R - self.v.get_value(obs_list[i]).squeeze(1)\n",
        "            v_loss = v_advantage.pow(2).mean()\n",
        "            v_loss.backward()\n",
        "\n",
        "        return self.pi.get_grad(), self.v.get_grad(), self.id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYqxDscqg39J"
      },
      "source": [
        "#Main\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsNoSNOGg8ZJ"
      },
      "source": [
        "num_workers = 2\n",
        "\n",
        "# Save device (cpu or cuda)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Make env\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# shape\n",
        "obs_shape = env.observation_space.shape\n",
        "act_shape = tuple([1]) # int to Tuple\n",
        "obs_dim = obs_shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# Define NN\n",
        "global_pi = CartpolePolicy(obs_dim=obs_dim, n_actions=n_actions, device=device)\n",
        "global_v  = CartpoleCritic(obs_dim=obs_dim, device=device)\n",
        "global_pi.to(device)\n",
        "global_v.to(device)\n",
        "\n",
        "# dummy step for making params.grad None to 0\n",
        "obs = env.reset()\n",
        "_, dummy_logprob = global_pi.get_action_logprob(obs)\n",
        "dummy_value = global_v.get_value(obs)\n",
        "dummy_logprob.backward()\n",
        "dummy_value.backward()\n",
        "\n",
        "# Optimizer\n",
        "global_pi_optimizer = optim.Adam(global_pi.parameters())\n",
        "global_v_optimizer = optim.Adam(global_v.parameters())\n",
        "\n",
        "rewards = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZbTaDosSzu6"
      },
      "source": [
        "@ray.remote로 annotation을 붙여준 class를 선언할 때는 .remote를 붙여줘서 사용할 수 있습니다.\n",
        "- 아래 코드에서는, agents에 num_workers만큼의 class를 worker로 선언하여 저장합니다.\n",
        "- gradient_list에는 선언한 agent들의, compute_gradient 함수의 operation을 저장합니다. (실행이 바로 되지는 않습니다.)\n",
        "\n",
        "\n",
        "그리고, for문을 돌면서,\n",
        "- ray.wait 함수를 통해 operation 중 가장 먼저 끝난 agent의 operation id(done_id)를 받고, 이를 ray.get 함수를 통해 실행시킨 뒤 그 반환값을 받아옵니다.\n",
        "- compute_gradient 함수로부터 gradient를 받고, 그 gradient를 global network에 적용시킵니다.\n",
        "\n",
        "- 또한, 한 번 연산이 끝나게 되면 gradient_list에서 해당 연산이 사라지게 되는데, 다시 연산이 추가될 수 있도록 연산을 append 시켜줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl2Q6cahhASO"
      },
      "source": [
        "agents = [A3CAgent.remote(\"PongNoFrameskip-v4\", i, device) for i in range(num_workers)]\n",
        "\n",
        "pi_params = global_pi.get_weights()\n",
        "v_params = global_v.get_weights()\n",
        "\n",
        "gradient_list = [agent.compute_gradient.remote(pi_params, v_params) for agent in agents]\n",
        "\n",
        "for step_i in range(1000):\n",
        "\n",
        "    done_id, gradient_list = ray.wait(gradient_list)\n",
        "    pi_grad, v_grad, id = ray.get(done_id)[0]\n",
        "    \n",
        "    global_pi_optimizer.zero_grad()\n",
        "    global_v_optimizer.zero_grad()\n",
        "\n",
        "    # set global pi & v gradient\n",
        "    global_pi.set_grad(pi_grad)\n",
        "    global_v.set_grad(v_grad)\n",
        "\n",
        "    # step\n",
        "    global_pi_optimizer.step()\n",
        "    global_v_optimizer.step()\n",
        "\n",
        "    gradient_list.append(agents[id].compute_gradient.remote(global_pi.get_weights(), global_v.get_weights()))\n",
        "\n",
        "    if step_i % 10 == 0:\n",
        "        test_reward = test(global_pi, env)\n",
        "        rewards.append(test_reward)\n",
        "        plot(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}