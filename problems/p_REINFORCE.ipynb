{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p_REINFORCE.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2tQ2KkO/PkTCks0/gCRYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keeeehun/RL/blob/main/problems/p_REINFORCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Mb7bFRLFe4"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Q network로 사용할 Neural Network 모델입니다.\n",
        "\n",
        "- 본 실습 때 사용할 CartPole 환경에서는 (4 -> 64 -> 128 -> 2)의 레이어 구조를 사용합니다.\n",
        "\n",
        "    - action이 뽑힐 확률을 받아야 하므로 forward 함수 마지막에 softmax를 취해줍니다.\n",
        "\n",
        "- get_action_logprob 함수를 통해 환경에 넣어줄 int형의 action과 action이 뽑힐 확률값(log probability)을 받을 수 있습니다.\n",
        "\n",
        "    - discrete action space이므로 Categorical 함수를 통해 Categorical 분포를 만들어줍니다."
      ],
      "metadata": {
        "id": "xFVBietzZAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CartpolePolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, device):\n",
        "        super(CartpolePolicy, self).__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        self.l1 = nn.Linear(obs_dim, 64)\n",
        "        self.l2 = nn.Linear(64, 128)\n",
        "        self.l3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        x = F.softmax(x, dim=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_action_logprob(self, obs):\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        obs = obs.to(self.device)\n",
        "        output = self.forward(obs)\n",
        "        '''\n",
        "        To do\n",
        "        '''\n",
        "        categorical = \n",
        "        action = \n",
        "        logprob = \n",
        "\n",
        "        return action.item(), logprob"
      ],
      "metadata": {
        "id": "Y-5WlzomLTQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n",
        "\n",
        "buffer에 저장해놓은 logprob과 return data를 사용해서 모델을 업데이트 하는 함수입니다.\n",
        "\n",
        "$$\\theta_{t+1} \\approx \\theta_t + \\alpha[\\partial_{θ}log\\pi_{\\theta}(a|s)G_t]$$ "
      ],
      "metadata": {
        "id": "s61cPb0sZEVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(logprobs, returns, optim):\n",
        "    optim.zero_grad()\n",
        "    # Cumulate gradients\n",
        "    for ret, logprob in zip(returns, logprobs):\n",
        "        '''\n",
        "        To do\n",
        "        '''\n",
        "        j = \n",
        "        j.backward()\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "cJX3FG3ULq4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matplotlib\n",
        "\n",
        "Matplotlib을 사용해서 테스트 reward와 TD error를 그래프로 보여주는 함수입니다."
      ],
      "metadata": {
        "id": "qEKce1-WZFDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    # plt.title('')\n",
        "    plt.ylabel('reward')\n",
        "    plt.xlabel('episode')\n",
        "    plt.plot(rewards)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aXBDVvORLspo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main\n",
        "\n",
        "트레이닝에 필요한 환경, 모델, loss, optimizer 등을 정의하는 코드입니다.\n",
        "\n",
        "logprob과 return을 저장해주기 위한 buffer를 따로 만들어줍니다."
      ],
      "metadata": {
        "id": "7vFR3iWvZHcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save device (cpu or cuda)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Make env\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# shape\n",
        "obs_shape = env.observation_space.shape\n",
        "act_shape = tuple([1]) # int to Tuple\n",
        "obs_dim = obs_shape[0]\n",
        "n_actions = env.action_space.n\n",
        "buffer_size = 1000\n",
        "\n",
        "# Define NN\n",
        "policy_model = CartpolePolicy(obs_dim=obs_dim, n_actions=n_actions, device=device)\n",
        "policy_model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(policy_model.parameters())\n",
        "\n",
        "# training parameters\n",
        "total_step = 60000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "\n",
        "# list for saving results\n",
        "rewards = []\n",
        "\n",
        "# list for saving return & logprobs\n",
        "logprobs = []\n",
        "returns = []"
      ],
      "metadata": {
        "id": "G2A1lSisLudb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REINFORCE 트레이닝을 실행하는 메인 루프입니다.\n",
        "\n",
        "- total_step만큼 돌면서 매 스탭마다 Buffer에 logprob과 reward를 넣어줍니다.\n",
        "- done 조건이 되면 저장된 reward로 각 시점마다 return 값을 계산해주고, model을 update 해줍니다."
      ],
      "metadata": {
        "id": "pUFonifpZJwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_reward = 0\n",
        "best_test_reward = -999999\n",
        "obs = env.reset()\n",
        "for t in range(total_step):\n",
        "    action, logprob = policy_model.get_action_logprob(obs)\n",
        "    next_obs, rew, done, _ = env.step(action)\n",
        "    logprobs.append(logprob)\n",
        "    returns.append(rew)\n",
        "    episode_reward += rew\n",
        "    obs = next_obs\n",
        "\n",
        "    if done:\n",
        "        # Save the best model so far\n",
        "        if best_test_reward <= episode_reward:\n",
        "            best_test_reward = episode_reward\n",
        "            torch.save(policy_model.state_dict(), \"policy_model.pt\")\n",
        "\n",
        "        # complete returns\n",
        "        for i in range(len(returns)-2, -1, -1):\n",
        "            returns[i] += returns[i+1]*gamma\n",
        "        rewards.append(episode_reward)\n",
        "        train(logprobs, returns, optimizer)\n",
        "        logprobs = []\n",
        "        returns = []\n",
        "        obs = env.reset()\n",
        "        episode_reward = 0\n",
        "    \n",
        "    if t % 1000 == 0:\n",
        "        plot(rewards)"
      ],
      "metadata": {
        "id": "ivoS-x7ULwpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym-notebook-wrapper"
      ],
      "metadata": {
        "id": "arOa4SuiLyFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gnwrapper\n",
        "import gym\n",
        "\n",
        "env = gnwrapper.LoopAnimation(gym.make('CartPole-v0'))\n",
        "\n",
        "obs = env.reset()\n",
        "for _ in range(500):\n",
        "    obs, rew, done, _ = env.step(env.action_space.sample()) # Take random action\n",
        "    env.render()\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "\n",
        "env.display()"
      ],
      "metadata": {
        "id": "dijVViAjL0IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_model.load_state_dict(torch.load(\"policy_model.pt\"))\n",
        "\n",
        "env = gnwrapper.LoopAnimation(gym.make('CartPole-v0'))\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for _ in range(500):\n",
        "    action, _ = policy_model.get_action_logprob(obs)  # Take action from trained model\n",
        "    env.render()\n",
        "    obs, rew, done, _ = env.step(action)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "\n",
        "env.display()"
      ],
      "metadata": {
        "id": "Pe8IrV3hL2fe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}