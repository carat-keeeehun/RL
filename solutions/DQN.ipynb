{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIvjD3VHpAjEzAKiW8hPpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keeeehun/RL/blob/main/solutions/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEsdnvLe3Egg"
      },
      "source": [
        "\"\"\"\n",
        "Reference\n",
        "https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwgReXvT37H-"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AzmJjND38_G"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G4rXCmY3_vT"
      },
      "source": [
        "# Replay Buffer\n",
        "\n",
        "Replay Buffer로 사용할 Class를 정의해줍니다.\n",
        "- store 함수를 통해 (S, A, R, S', done)을 저장합니다.\n",
        "- sample_batch 함수를 통해 batch size만큼의 history data를 dictonary 형태로 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2keqpi3_O0"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_shape, act_shape, buffer_size):\n",
        "        buffer_obs_shape = tuple([buffer_size]) + obs_shape\n",
        "        buffer_act_shape = tuple([buffer_size]) + act_shape\n",
        "        self.obs_buf = np.zeros(buffer_obs_shape, dtype=np.float32)\n",
        "        self.obs2_buf = np.zeros(buffer_obs_shape, dtype=np.float32)\n",
        "        self.act_buf = np.zeros(buffer_act_shape, dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.done_buf = np.zeros(buffer_size, dtype=np.float32)\n",
        "        self.pointer, self.size, self.buffer_size = 0, 0, buffer_size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        self.obs_buf[self.pointer] = obs\n",
        "        self.obs2_buf[self.pointer] = next_obs\n",
        "        self.act_buf[self.pointer] = act\n",
        "        self.rew_buf[self.pointer] = rew\n",
        "        self.done_buf[self.pointer] = done\n",
        "        self.pointer = (self.pointer+1) % self.buffer_size\n",
        "        self.size = min(self.size+1, self.buffer_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        batch = dict(obs=self.obs_buf[idxs],\n",
        "                     obs2=self.obs2_buf[idxs],\n",
        "                     act=self.act_buf[idxs],\n",
        "                     rew=self.rew_buf[idxs],\n",
        "                     done=self.done_buf[idxs])\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL-EGnlL4bSA"
      },
      "source": [
        "# Model\n",
        "\n",
        "Q network로 사용할 Neural Network 모델입니다.\n",
        "\n",
        "- 본 실습 때 사용할 CartPole 환경에서는 (4 -> 64 -> 128 -> 2)의 레이어 구조를 사용합니다.\n",
        "\n",
        "- get_action 함수를 통해 환경에 넣어줄 int형의 action을 받을 수 있습니다.\n",
        "\n",
        "- get_output 함수를 통해 model의 output을 받을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfeUqUEp4anO"
      },
      "source": [
        "class DQNCartpole(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, device):\n",
        "        super(DQNCartpole, self).__init__()\n",
        "\n",
        "        self.obs_dim = obs_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        self.l1 = nn.Linear(obs_dim, 64)\n",
        "        self.l2 = nn.Linear(64, 128)\n",
        "        self.l3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_action(self, obs, epsilon=0):\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        obs = obs.to(self.device)\n",
        "\n",
        "        # epsilon greedy\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                q = self.forward(obs)\n",
        "                action = torch.argmax(q).item()\n",
        "        else:\n",
        "            action = random.randrange(self.n_actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_output(self, obs, need_grad=True):\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        obs = obs.to(self.device)\n",
        "        if need_grad:\n",
        "            output = self.forward(obs)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                output = self.forward(obs)\n",
        "\n",
        "        return output.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UOHILVC5hGy"
      },
      "source": [
        "# Test\n",
        "\n",
        "모델과 환경을 파라미터로 받아서 하나의 에피소드를 테스트하는 함수입니다.\n",
        "\n",
        "해당 에피소드의 리워드를 구해서 반환해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBuwXnhY43qg"
      },
      "source": [
        "def test(q_model, env):\n",
        "    test_episode_reward = 0\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action = q_model.get_action(obs, 0)\n",
        "        next_obs, rew, done, _ = env.step(action)\n",
        "        test_episode_reward += rew\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return test_episode_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZerSc-RP5rkx"
      },
      "source": [
        "# Train\n",
        "\n",
        "Replay buffer에서 가져온 batch data를 사용해서 모델을 업데이트 하는 함수입니다.\n",
        "\n",
        "target q value와 q value 사이의 TD error를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC74GXNU45Tp"
      },
      "source": [
        "def train(q_model, target_q_model, batch, optim, gamma):\n",
        "    obs = batch[\"obs\"].to(device)\n",
        "    next_obs = batch[\"obs2\"].to(device)\n",
        "    act = torch.tensor(batch[\"act\"], dtype=torch.int64).to(device)\n",
        "    rew = batch[\"rew\"].to(device)\n",
        "    done = batch[\"done\"].to(device)\n",
        "\n",
        "    obs_output = q_model(obs)\n",
        "    q = torch.gather(obs_output, 1, act).squeeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_obs_output = target_q_model(next_obs)\n",
        "        next_q = torch.max(next_obs_output, dim=1)[0]\n",
        "\n",
        "    td_target = rew + gamma * next_q * (1-done)\n",
        "    td_error = (q - td_target).pow(2).mean()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    td_error.backward()\n",
        "    optim.step()\n",
        "\n",
        "    return td_error.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYLJZomP55Dk"
      },
      "source": [
        "# Matplotlib\n",
        "\n",
        "Matplotlib을 사용해서 테스트 reward와 TD error를 그래프로 보여주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3iB8Z7T47uh"
      },
      "source": [
        "def plot(test_rewards, td_errors):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('test reward')\n",
        "    plt.plot(test_rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('TD error')\n",
        "    plt.plot(td_errors)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEgiVAIG6B_O"
      },
      "source": [
        "# Main\n",
        "\n",
        "트레이닝에 필요한 환경, 모델, loss, optimizer 등을 정의하는 코드입니다.\n",
        "\n",
        "target q model은 q model과 같아지도록 복사해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhk5HWcp4-yP"
      },
      "source": [
        "# Save device (cpu or cuda)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Make env\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# shape\n",
        "obs_shape = env.observation_space.shape\n",
        "act_shape = tuple([1]) # int to Tuple\n",
        "obs_dim = obs_shape[0]\n",
        "n_actions = env.action_space.n\n",
        "buffer_size = 1000\n",
        "\n",
        "print(f\" State dimension : {obs_dim}\")\n",
        "print(f\" number of Actions : {n_actions}\")\n",
        "\n",
        "# Define NN\n",
        "q_model = DQNCartpole(obs_dim=obs_dim, n_actions=n_actions, device=device)\n",
        "target_q_model = DQNCartpole(obs_dim=obs_dim, n_actions=n_actions, device=device)\n",
        "q_model.to(device)\n",
        "target_q_model.to(device)\n",
        "\n",
        "# Copy weight\n",
        "target_q_model.load_state_dict(q_model.state_dict())\n",
        "\n",
        "# Buffer & optimizer\n",
        "replay_buffer = ReplayBuffer(obs_shape, act_shape, buffer_size)\n",
        "optimizer = optim.Adam(q_model.parameters())\n",
        "\n",
        "# training parameters\n",
        "total_step = 10000\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "epsilon = 0.05\n",
        "copy_freq = 200\n",
        "\n",
        "# lists for saving results\n",
        "test_rewards = []\n",
        "td_errors = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYOq_Pr6Qr8"
      },
      "source": [
        "DQN 트레이닝을 실행하는 메인 루프입니다.\n",
        "\n",
        "- total_step만큼 돌면서 매 스탭마다 (S, A, R, S', done)을 Replay Buffer에 store 함수를 통해 넣어줍니다.\n",
        "- Replay Buffer의 len이 batch size 이상이 되면 sample_batch 함수를 통해 history를 불러와서 한번 모델 업데이트를 실행합니다.\n",
        "- Test는 매 50스탭마다 진행하며, 200 스탭마다 target q model을 업데이트 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFKTMI-A5EAl"
      },
      "source": [
        "episode_reward = 0\n",
        "best_test_reward = -999999\n",
        "obs = env.reset()\n",
        "for t in range(total_step):\n",
        "    action = q_model.get_action(obs, epsilon)\n",
        "    next_obs, rew, done, _ = env.step(action)\n",
        "    replay_buffer.store(obs, action, rew, next_obs, done)\n",
        "    episode_reward += rew\n",
        "    obs = next_obs\n",
        "\n",
        "    if done:\n",
        "        print(\"episode_reward :\", episode_reward)\n",
        "        obs = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "    # Train\n",
        "    if len(replay_buffer) > batch_size:\n",
        "        sampled_batch = replay_buffer.sample_batch(batch_size=batch_size)\n",
        "        td_error = train(q_model, target_q_model, sampled_batch, optimizer, gamma)\n",
        "        if t % 50 == 0:\n",
        "            td_errors.append(td_error)\n",
        "    \n",
        "    # Test\n",
        "    if t % 50 == 0:\n",
        "        test_reward = test(q_model, gym.make(\"CartPole-v0\"))\n",
        "        test_rewards.append(test_reward)\n",
        "        plot(test_rewards, td_errors)\n",
        "        # print(\"test reward :\", test_reward)\n",
        "        if best_test_reward <= test_reward:\n",
        "            # print(\"saved\")\n",
        "            best_test_reward = test_reward\n",
        "            torch.save(q_model.state_dict(), \"q_model.pt\")\n",
        "            torch.save(target_q_model.state_dict(), \"target_q_model.pt\")\n",
        "\n",
        "    if t % copy_freq == 0:\n",
        "        target_q_model.load_state_dict(q_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}